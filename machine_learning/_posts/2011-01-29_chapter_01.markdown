Chapter 01
====

Checkers Learning Problem
----
* (T)ask T: playing checkers
* (P)ermformance measure P: % of games won
* Training (E)xperience E:

System Design
----
1. knowledge to be learned
2. representation of knowledge
3. learning mechanism

Knowledge to be Learned - Choosing a Target Function
----
The knowledge we want to learn is the best legal move.  We can best represent this as a function or program.  One common strategy is to reduce improving performance P at task T to the problem of learning a *target function*.

Let's try using this as our terget function:
ChooseMove: B -> M

def choose_move(board_state)
	return best_move

An alternative to ChooseMove would a function that scores a given board state:
V: B -> R

def V(board_state)
	return score

What should V return for a board state?  Although any number higher than another number could reperesent a board state, we should create some specifications:

1. final board state is won, V(b) = 100
2. final board state is lost, V(b) = -100
3. final board state is tied, V(b) = 0
4. not final board state, V(b) = the best final board state that we can get to starting from the original board state, b

1-3 is trivial, 4 is not because it requires going through all possible subsequent board states to find the best possible final board state.  It is not efficiently computable.

nonoperational definition - not efficiently computable
operational definition - compute within realistic time bounds

Consequently, we'll settle for an approximation of our target function, V.

V^ - the function that we actually learn (the approximation... could be close!)

In conclusion, we're trying to learn V^, the approximation of the ideal function that returns a score for any given board state

Representation of Knowlege - Choosing a Representation of the Target Function
----
Some options for representing V^
* table of board states
* set of rules that rate the board state
* a polynomial function representing board features
 
When choosing a representation, the expressiveness of the representation versus the amount of training data needed for the representation must be taken into consideration.

For the sample checkers learning program a simple linear combination could be used to describe weighted board features such as number of pieces on the board, pieces threatened, etc... which would result in something like:

V^(b) = w0 + w1x1 + w2x2 + .... wnxn

The problem is now reduced to finding the values for the weight coefficients.

Learning Mechanism - Choosing a Function Approximation Algorithm
----

What does our training data look like?  It should be a tuple consisting of the board state (as represented by our target function), and the value of that board state:
((x1=3, x2=0.... xn=m), +100)

Estimating Training Values
----
How do we estimate training values based on the eventual outcome of the game?  This is trivial for end game conditions.  However, intermediate board states cannot be easily estimated.  One strategy is to use the current approximation of the training function, V^, to calculate the value for the board state after the current one:

Vtrain(b) <- V(Successor(b))

Adjusting Weights
----
One strategy is to minimize the square of the difference between the training value, vtrain(b), at board state, b, and the value of the current approximate function, V^ at board state b:

E (error) = every tuple of training examples, sum... (Vtrain(b) - V^(b))^2

Lease Mean Square Rule
----
For each training example (b, vtrain(b))

* Use the current weights to calculate V^(b)
* For each weight update it... wi <- wi + n(Vtrain(b) - V^(b))xi

That is weight is equal to the current weight + some small constant, n,  times the difference between the training value and the approximate value at board state, b.... and then times xi

This means when the error (Vtrain(b) - V^(b)) is zero, then nothing is added to the weight!

Final Design of Program
----
4 components:
1. Performance System -> takes a problem as input, plays through it and outputs the history
2. Critic -> Goes through game history and transforms moves into training examples (scores moves)
3. Generalizer -> Takes the training examples to come with a new V... using the LMS algo... output is new V^
4. Experiment Generator -> takes current hypothesis (V^) and creates a new problem

Perspectives in Machine Learning
____
Mostly about searching through some large number of hypotheses!

Usually there is some sort of underlying representation of the search space, like a linear equation or a neural network.

Learning is a search problem.

1.1
----
Applications
1. OCR -> needs to adjust for variations in text
2. Natural language processing -> natural language can be ambiguous, need large training set to pick nuances
3. User recommendations -> based on past choices, predict future choices
Not Applications
Anything where an algo is already known...
1. Calculator!
2. Shortest path
3. CRUD applications (straight data storage)

1.2
----
Describe informally and formally... OCR.
Take handwritten text and recognize the characters in it.

Task T: identify characters
Performance: % correctly identified characters
Training Experience:  handwriting samples

Target function: proximity, nearest neighbor
Target representation: matrix (bitmap!)
